{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import requests\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a pretrained torchvision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Write a function to load a pretrained object detection model from torchvision in eval mode\n",
    "\n",
    "def load_model():\n",
    "    # ...\n",
    "    # write code here\n",
    "    # ...\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get images to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \"https://www.sfmta.com/sites/default/files/imce-images/2021/pedestrian_scramble.jpg\" -o pedestrian_scramble.jpg\n",
    "!curl \"https://static.wixstatic.com/media/0b1913_a8d6b79a2f624015b42ecf5b8efa54fc~mv2.jpg\" -o cats.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(\"pedestrian_scramble.jpg\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Write a function that accepts the image file path and returns a tensor\n",
    "\n",
    "def load_as_tensor(img_path: str):\n",
    "    # ...\n",
    "    # write code to Load as PIL image\n",
    "    # write code to Convert PIL image to tensor\n",
    "    # ...\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = load_as_tensor(\"pedestrian_scramble.jpg\")\n",
    "print(img1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = load_as_tensor(\"cats.jpg\")\n",
    "print(img2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchify\n",
    "- Since the operations on each image are identical and independent of each other, they can be performed in parallel. This is why inputs to deep learning models are batches of images (or text or audio or whatever your model consumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all images of a batch\n",
    "batch = [img1, img2]\n",
    "\n",
    "# Convert list to tensor\n",
    "input_batch = torch.stack(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! You just got an error! Don't fret, let's figure out what went wrong...\n",
    "\n",
    "The stacktrace says we couldn't create a batch because the image sizes are different.\n",
    "\n",
    "When sizes are different, the operations are no longer identical (large images will need more operations). For parallel processing, the batch must contain images of the same size.\n",
    "\n",
    "In the real world, it's likely we won't always get images of the same size. To resize our images, we can use `torchvision.transforms.Resize` in our preprocessing function. Let's try that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the preprocessing function\n",
    "\n",
    "Rewrite the preprocessing function from above so that after the image is loaded as a tensor, we resize it to 224 pixels in height and width.\n",
    "\n",
    "Use [torchvision.transforms.Resize](https://pytorch.org/vision/main/generated/torchvision.transforms.Resize.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Update `load_as_tensor` to resize the image tensor to 224x224\n",
    "\n",
    "def load_as_tensor(img_path: str):\n",
    "    # ...\n",
    "    # write code to Load as PIL image\n",
    "    # write code to Convert PIL image to tensor\n",
    "    # write code to resize image tensor\n",
    "    # ...\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it! Make sure the image tensor sizes are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = load_as_tensor(\"pedestrian_scramble.jpg\")\n",
    "print(img1.size())\n",
    "\n",
    "img2 = load_as_tensor(\"cats.jpg\")\n",
    "print(img2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why (3, 224, 224)? It is the smallest permissible image that we can use with pretrained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchify... again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [img1, img2]\n",
    "input_batch = torch.stack(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: What is the size of the `input_batch` tensor?\n",
    "\n",
    "# ...\n",
    "# write code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input batch tensor resembles the classic (N, C, H, W) format you will encounter often in your computer vision journey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference on the image\n",
    "Pass the input batch through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: How many elements does `predictions` contain? \n",
    "\n",
    "# ...\n",
    "# write code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the number of elements in `prediction` relate to the number of images in the input batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Explore what each prediction contains. What do you think all these numbers mean?\n",
    "\n",
    "# ...\n",
    "# write code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has returned to us 3 things:\n",
    "- boxes: coordinates of the bounding boxes around detected objects\n",
    "- labels: what it thinks the detected object is \n",
    "- scores: confidence in the predicted label (ranging from 0 - 1, higher is more confident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: See what objects have been detected in the first image\n",
    "\n",
    "# ...\n",
    "# write code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: What are the scores of the most-confident and least-confident predictions?\n",
    "\n",
    "# ...\n",
    "# write code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-process output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has given us integers for labels. These integers are indices that map to object names in the CoCo dataset.\n",
    "\n",
    "Here's a function to load the lookup map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping_dict():\n",
    "    idx_to_labels_url = \"https://gist.githubusercontent.com/suraj813/1fe4c9dd0bc7e1dd1ce79462712ac9ce/raw/0e2c65813946769a375d673a34a1c0236b0505f1/coco_idx_to_labels.txt\"\n",
    "    r = requests.get(idx_to_labels_url).text\n",
    "    map = {int(k) : v for k,v in ast.literal_eval(r).items()}\n",
    "    return map\n",
    "\n",
    "label_lookup = get_mapping_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out! `1` seems to a common label in the first image, what does it correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: What is the object the model predicts as `1`?\n",
    "\n",
    "# ...\n",
    "# write code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can translate the model's output labels to actual object names, let's try to build a report for each image.\n",
    "\n",
    "The report should contain all the objects in the image BUT the model isn't confident about every prediction it has made. We should ignore predictions below a certain threshold.\n",
    "\n",
    "There might be multiple occurences of an object in the image; instead of listing every occurrence of the object, the report can just contain an aggregate count of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_detection_report(model_output, confidence_threshold=0.8):\n",
    "    # ...\n",
    "    # write code here\n",
    "    # ...\n",
    "\n",
    "    # HINTS\n",
    "    # - Unpack the output dictionary to get the bbox, labels, and confidence values\n",
    "    # - Convert the labels and confidence arrays to lists for easier processing\n",
    "    # - Get a lookup dictionary for the class labels\n",
    "    # - Loop through each label and its corresponding confidence value. Record only those predictions above \n",
    "    #   the confidence threshold\n",
    "    # - Use a Counter object to count the number of times each class appears in the detected_objects list\n",
    "    # - Return a tuple containing the list of detected objects and the class counts\n",
    "    \n",
    "    return detected_objects, counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, pred in enumerate(predictions):\n",
    "    detected_objects, counts = create_detection_report(pred, confidence_threshold=0.85)   \n",
    "\n",
    "    print(f\"Objects detected in image {c+1}:\\n\", \"=\"*20)\n",
    "    pprint(detected_objects)\n",
    "    print()\n",
    "\n",
    "    print(\"Count of objects:\\n\", \"=\"*20)\n",
    "    pprint(counts)\n",
    "    \n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take-home assignment\n",
    "\n",
    "Improve this report by drawing boxes on the input image and labelling each box with the detected object and confidence score.\n",
    "\n",
    "HINT: https://pytorch.org/vision/main/generated/torchvision.utils.draw_bounding_boxes.html"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
