{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop instructs how to use PyTorch to detect objects within an image. \n",
    "\n",
    "Upon completion you will have a basic understanding of:\n",
    "\n",
    "1. Image loading and manipulation in Python and PyTorch\n",
    "2. Loading pretrained models with Torchvision \n",
    "3. Batch processing in deep learning models\n",
    "4. Inference and post-processing with object detection models \n",
    "\n",
    "**Note:** This file is intended to be run on [Google Colab](https://colab.research.google.com). If you're viewing this file on github, [click here](https://githubtocolab.com/fbsamples/mit-dl-workshop/blob/main/object-detection/exercise.ipynb) to load it into google colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this workshop import the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import requests\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the required libraries loaded, you will need to create a device for training. The GPU is more efficient than the CPU, but it may not always be available. The following code will use the GPU if it's available. Otherwise, it will use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load a pretrained torchvision model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've created the device, it's time to load a pretrained object detection model. For this workshop, you will use the `fasterrccn_resnet50_fpn`. It's built into `torchvision`, so it became immediately available to you when you imported it above.\n",
    "\n",
    "So how do you load it? Like this `torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)`\n",
    "\n",
    "For this section you will update the `load_model` function by replacing `# write code here` with the coad to load faster rcnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Write a function to load a pretrained object detection model from torchvision in eval mode\n",
    "\n",
    "def load_model():\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # Set it to `eval` mode because we aren't training the model\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test `load_model` using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = load_model()\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÜüèÜ Do you see \"Downloading\" and \"100% 160M/160M [00:09<00:00, 20.3MB/s]\" in the output above? That means you have downloaded the model's pretrained weights. Congrats, you have successfully completed this exercise! üèÜüèÜ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get images to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've got the model trained, it's time to source some images to detect objects from. We've prepared two for this workshop. \n",
    "\n",
    "To download them you will use the curl command. It's a tool for transferring data from or to a server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  188k  100  188k    0     0   132k      0  0:00:01  0:00:01 --:--:--  132k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 17.0M  100 17.0M    0     0  5079k      0  0:00:03  0:00:03 --:--:-- 5085k\n"
     ]
    }
   ],
   "source": [
    "!curl \"https://www.sfmta.com/sites/default/files/imce-images/2021/pedestrian_scramble.jpg\" -o pedestrian_scramble.jpg\n",
    "!curl \"https://static.wixstatic.com/media/0b1913_a8d6b79a2f624015b42ecf5b8efa54fc~mv2.jpg\" -o cats.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've downloaded the images, try opening it and previewing it using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('pedestrian_scramble.jpg', width=240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preprocess the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have two images, you will need to preprocess them before you can have the model detect objects within them. To do that you will need to convert the images into a tensor. \n",
    "- PIL (Python Imaging Library) contains helper functions to read, manipulate and write images from disk. \n",
    "- TorchVision includes a `transforms` class that you can use to convert PIL objects into tensors.\n",
    "\n",
    "For this exercise you will need to write the `load_as_tensor` function that:\n",
    "\n",
    "1. Loads an image path as a PIL object. \n",
    "2. Transform it to a tensor.\n",
    "\n",
    "To load image you can use `Image.open`. Once opened you can use `torchvision.transforms.ToTensor` to convert it to a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Write a function that accepts the image file path and returns a tensor\n",
    "\n",
    "def load_as_tensor(img_path):\n",
    "    image = Image.open(img_path) # Load as PIL image\n",
    "    image = torchvision.transforms.ToTensor()(image) # Convert PIL image to tensor\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've completed the function, try loading the images one by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 806, 1200])\n"
     ]
    }
   ],
   "source": [
    "img1 = load_as_tensor(\"pedestrian_scramble.jpg\")\n",
    "print(img1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5074, 5074])\n"
     ]
    }
   ],
   "source": [
    "img2 = load_as_tensor(\"cats.jpg\")\n",
    "print(img2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÜüèÜ Do you see torch.Size([3, 806, 1200]) and torch.Size([3, 5074, 5074])? If so, congrats! üèÜüèÜ\n",
    "\n",
    "Now it's time to start detecting objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Batchify\n",
    "\n",
    "Our example only includes 2 images, but in the real world it's not uncommon to process thousands to millions of images. Processing each image one at a time is incredibly inefficient (especially with modern GPU memory capacities) and slow. \n",
    "\n",
    "Is there a way to speed this up? Yes! The answer is to batchify it! \n",
    "\n",
    "The operations on each image are identical and independent of each other, so they can be performed in parallel. This is why inputs to deep learning models are batches of images (or text or audio or whatever your model consumes).\n",
    "\n",
    "To batchify your images create an array of images and convert it to a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 806, 1200] at entry 0 and [3, 5074, 5074] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/subramen/META/mit-dl-workshop/notebooks/object_detection.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/subramen/META/mit-dl-workshop/notebooks/object_detection.ipynb#ch0000035?line=0'>1</a>\u001b[0m batch \u001b[39m=\u001b[39m [img1, img2]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/subramen/META/mit-dl-workshop/notebooks/object_detection.ipynb#ch0000035?line=1'>2</a>\u001b[0m input_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack(batch)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 806, 1200] at entry 0 and [3, 5074, 5074] at entry 1"
     ]
    }
   ],
   "source": [
    "# Create list of all images of a batch\n",
    "batch = [img1, img2]\n",
    "\n",
    "# Convert list to tensor\n",
    "input_batch = torch.stack(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! You just got an error! Don't fret, let's figure out what went wrong...\n",
    "\n",
    "The stacktrace says we couldn't create a batch because the image sizes are different.\n",
    "\n",
    "When sizes are different, the operations are no longer identical (large images will need more operations). For parallel processing, the batch must contain images of the same size.\n",
    "\n",
    "Also, in the real world it's unlikely to always get images of the same size. Our preprocessing function should also resize images to the same size. We can use `torchvision.transforms.Resize` in our preprocessing function. Let's try that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Update the preprocessing function\n",
    "\n",
    "Rewrite the preprocessing function from above so that after the image is loaded as a tensor and resize it to 224 pixels in height and width.\n",
    "\n",
    "Use [torchvision.transforms.Resize](https://pytorch.org/vision/main/generated/torchvision.transforms.Resize.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Update `load_as_tensor` to resize the image tensor to 224x224\n",
    "\n",
    "def load_as_tensor(img_path):\n",
    "    image = Image.open(img_path) # Load as PIL image\n",
    "    image = torchvision.transforms.ToTensor()(image) # Convert PIL image to tensor\n",
    "    image = torchvision.transforms.Resize(size=(224,224))(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've resized the tensor, test it to make sure the image tensor sizes are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "img1 = load_as_tensor(\"pedestrian_scramble.jpg\")\n",
    "print(img1.size())\n",
    "\n",
    "img2 = load_as_tensor(\"cats.jpg\")\n",
    "print(img2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why did we choose 224 x 224 for an image of size  (3, 224, 224) \n",
    "\n",
    "**Answer:** It is the smallest permissible image that pretrained models support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Batchify... again\n",
    "\n",
    "Now that you've updated `load_as_tensor` to resize images to 224 x 224, try batching them again. That pesky error message should go away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [img1, img2]\n",
    "input_batch = torch.stack(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: What is the size of the `input_batch` tensor?\n",
    "\n",
    "print(input_batch.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÜüèÜ Did you get *torch.Size([2, 3, 224, 224])*? If so, congrats! üèÜüèÜ\n",
    "\n",
    "The input batch tensor resembles the classic (N, C, H, W) format you will encounter often in your computer vision journey.\n",
    "\n",
    "N: Number of images \n",
    "\n",
    "C: Channels (like RGB, or CMYK)\n",
    "\n",
    "H: Height  \n",
    "\n",
    "W: Width  \n",
    "\n",
    "Now it's time to ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Run inference on the image\n",
    "Pass the input batch through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: How many elements does `predictions` contain? \n",
    "\n",
    "print(len(predictions))  # must match input batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the number of elements in `prediction` relate to the number of images in the input batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['boxes', 'labels', 'scores'])\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: Explore what each prediction contains. What do you think all these numbers mean?\n",
    "\n",
    "p0 = predictions[0]\n",
    "\n",
    "# print(p0)\n",
    "print(p0.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returns 3 things:\n",
    "- boxes: coordinates of the bounding boxes around detected objects\n",
    "- labels: what it thinks the detected object is \n",
    "- scores: confidence in the predicted label (ranging from 0 - 1, higher is more confident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  3, 10,  8,  3,  1,  1,  1,  1,  3,  1,  1,  1,  3, 10, 10,  6,  3,\n",
       "         1,  6,  3,  1,  3,  1,  8,  1,  1,  1,  3,  1,  1, 10,  6,  3,  8, 10,\n",
       "         3,  3,  1,  1,  3, 10,  3,  8,  3,  8,  3,  1,  3,  1, 10,  3,  6,  1,\n",
       "         8,  1,  1,  1,  3,  6,  6,  1,  8,  3,  3,  1,  3, 10,  1,  1,  1,  1,\n",
       "        31,  6, 10,  1,  3,  6,  1, 33,  3,  1, 10, 41, 27,  1, 14, 31,  8,  8,\n",
       "         3,  8,  6, 31,  6,  2,  1,  3,  1,  6])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXERCISE: See what objects have been detected in the first image\n",
    "\n",
    "p0['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of most-confident prediction:  0.973721981048584\n",
      "Score of least-confident prediction:  0.05608990788459778\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: What are the scores of the most-confident and least-confident predictions?\n",
    "\n",
    "print(\"Score of most-confident prediction: \", max(p0['scores']).item())\n",
    "print(\"Score of least-confident prediction: \", min(p0['scores']).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Post-process output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has given us integers for labels. These integers are indices that map to object names in the CoCo dataset.\n",
    "\n",
    "Here's a function to load the lookup map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping_dict():\n",
    "    idx_to_labels_url = \"https://gist.githubusercontent.com/suraj813/1fe4c9dd0bc7e1dd1ce79462712ac9ce/raw/0e2c65813946769a375d673a34a1c0236b0505f1/coco_idx_to_labels.txt\"\n",
    "    r = requests.get(idx_to_labels_url).text\n",
    "    map = {int(k) : v for k,v in ast.literal_eval(r).items()}\n",
    "    return map\n",
    "\n",
    "label_lookup = get_mapping_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out! `1` seems to a common label in the first image, what does it correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: What is the object the model predicts as `1`?\n",
    "\n",
    "print(label_lookup[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Build a report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how to  translate the model's output labels to actual object names, try to build a report for each image.\n",
    "\n",
    "The report should contain all the objects in the image BUT the model isn't confident about every prediction it has made. So you should ignore predictions below a certain threshold.\n",
    "\n",
    "There might be multiple occurences of an object in the image; instead of listing every occurrence of the object, the report can just contain an aggregate count of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_detection_report(model_output, confidence_threshold=0.8):\n",
    "    # Unpack the output dictionary to get the bbox, labels, and confidence values\n",
    "    bbox, labels, confidence = model_output.values()\n",
    "    \n",
    "    # Convert the labels and confidence arrays to lists for easier processing\n",
    "    labels = labels.tolist()\n",
    "    confidence = confidence.tolist()\n",
    "\n",
    "    # Get a lookup dictionary for the class labels\n",
    "    label_lookup = get_mapping_dict()\n",
    "\n",
    "    # Loop through each label and its corresponding confidence value\n",
    "    detected_objects = []\n",
    "    for label, confidence in zip(labels, confidence):\n",
    "        # Check if the confidence value is above the threshold\n",
    "        if confidence > confidence_threshold:\n",
    "            # Use the label lookup to get the class name and add it to the list of detected objects\n",
    "            classname = label_lookup[label]\n",
    "            detected_objects.append((classname, confidence,))\n",
    "    \n",
    "    # Use a Counter object to count the number of times each class appears in the detected_objects list\n",
    "    counts = Counter([x[0] for x in detected_objects])\n",
    "\n",
    "    # Return a tuple containing the list of detected objects and the class counts\n",
    "    return detected_objects, counts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects detected in image 1:\n",
      " ====================\n",
      "[('car', 0.973721981048584),\n",
      " ('car', 0.9571205973625183),\n",
      " ('traffic light', 0.9557090401649475),\n",
      " ('truck', 0.9524416327476501),\n",
      " ('car', 0.9516623020172119),\n",
      " ('person', 0.9422258734703064),\n",
      " ('person', 0.9256057739257812),\n",
      " ('person', 0.9153063893318176),\n",
      " ('person', 0.8720185160636902),\n",
      " ('car', 0.8719595074653625),\n",
      " ('person', 0.863501250743866)]\n",
      "\n",
      "Count of objects:\n",
      " ====================\n",
      "Counter({'person': 5, 'car': 4, 'traffic light': 1, 'truck': 1})\n",
      "\n",
      "\n",
      "\n",
      "Objects detected in image 2:\n",
      " ====================\n",
      "[('cat', 0.9777542352676392), ('cat', 0.967362105846405)]\n",
      "\n",
      "Count of objects:\n",
      " ====================\n",
      "Counter({'cat': 2})\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c, pred in enumerate(predictions):\n",
    "    detected_objects, counts = create_detection_report(pred, confidence_threshold=0.85)   \n",
    "\n",
    "    print(f\"Objects detected in image {c+1}:\\n\", \"=\"*20)\n",
    "    pprint(detected_objects)\n",
    "    print()\n",
    "\n",
    "    print(\"Count of objects:\\n\", \"=\"*20)\n",
    "    pprint(counts)\n",
    "    \n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take-home assignment\n",
    "\n",
    "Improve this report by drawing boxes on the input image and labelling each box with the detected object and confidence score.\n",
    "\n",
    "HINT: https://pytorch.org/vision/main/generated/torchvision.utils.draw_bounding_boxes.html"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
