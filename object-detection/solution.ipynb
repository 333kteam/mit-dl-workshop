{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import requests\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a pretrained torchvision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Write a function to load a pretrained object detection model from torchvision in eval mode\n",
    "\n",
    "def load_model():\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # Set it to `eval` mode because we aren't training the model\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = load_model()\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get images to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  188k  100  188k    0     0   132k      0  0:00:01  0:00:01 --:--:--  132k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 17.0M  100 17.0M    0     0  5079k      0  0:00:03  0:00:03 --:--:-- 5085k\n"
     ]
    }
   ],
   "source": [
    "!curl \"https://www.sfmta.com/sites/default/files/imce-images/2021/pedestrian_scramble.jpg\" -o pedestrian_scramble.jpg\n",
    "!curl \"https://static.wixstatic.com/media/0b1913_a8d6b79a2f624015b42ecf5b8efa54fc~mv2.jpg\" -o cats.jpg\n",
    "\n",
    "# !wget \"https://www.sfmta.com/sites/default/files/imce-images/2021/pedestrian_scramble.jpg\" -O pedestrian_scramble.jpg\n",
    "# !wget \"https://static.wixstatic.com/media/0b1913_a8d6b79a2f624015b42ecf5b8efa54fc~mv2.jpg\" -O cats.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(\"pedestrian_scramble.jpg\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Write a function that accepts the image file path and returns a tensor\n",
    "\n",
    "def load_as_tensor(img_path):\n",
    "    image = Image.open(img_path) # Load as PIL image\n",
    "    image = torchvision.transforms.ToTensor()(image) # Convert PIL image to tensor\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 806, 1200])\n"
     ]
    }
   ],
   "source": [
    "img1 = load_as_tensor(\"pedestrian_scramble.jpg\")\n",
    "print(img1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5074, 5074])\n"
     ]
    }
   ],
   "source": [
    "img2 = load_as_tensor(\"cats.jpg\")\n",
    "print(img2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchify\n",
    "- Since the operations on each image are identical and independent of each other, they can be performed in parallel. This is why inputs to deep learning models are batches of images (or text or audio or whatever your model consumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 806, 1200] at entry 0 and [3, 5074, 5074] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/subramen/META/mit-dl-workshop/notebooks/object_detection.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/subramen/META/mit-dl-workshop/notebooks/object_detection.ipynb#ch0000035?line=0'>1</a>\u001b[0m batch \u001b[39m=\u001b[39m [img1, img2]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/subramen/META/mit-dl-workshop/notebooks/object_detection.ipynb#ch0000035?line=1'>2</a>\u001b[0m input_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack(batch)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 806, 1200] at entry 0 and [3, 5074, 5074] at entry 1"
     ]
    }
   ],
   "source": [
    "batch = [img1, img2]\n",
    "input_batch = torch.stack(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! You just got an error! Don't fret, let's figure out what went wrong...\n",
    "\n",
    "The stacktrace says we couldn't create a batch because the image sizes are different.\n",
    "\n",
    "When sizes are different, the operations are no longer identical (large images will need more operations). For parallel processing, the batch must contain images of the same size.\n",
    "\n",
    "In the real world, it's likely we won't always get images of the same size. To resize our images, we can use `torchvision.transforms.Resize` in our preprocessing function. Let's try that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the preprocessing function\n",
    "\n",
    "Rewrite the preprocessing function from above so that after the image is loaded as a tensor, we resize it to 224 pixels in height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERVISE: Update `load_as_tensor` to resize the image tensor to 224x224\n",
    "\n",
    "def load_as_tensor(img_path):\n",
    "    image = Image.open(img_path) # Load as PIL image\n",
    "    image = torchvision.transforms.ToTensor()(image) # Convert PIL image to tensor\n",
    "    image = torchvision.transforms.Resize(size=(224,224))(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it! Make sure the image tensor sizes are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "img1 = load_as_tensor(\"pedestrian_scramble.jpg\")\n",
    "print(img1.size())\n",
    "\n",
    "img2 = load_as_tensor(\"cats.jpg\")\n",
    "print(img2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why (3, 224, 224)? It is the smallest permissible image that we can use with pretrained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchify... again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [img1, img2]\n",
    "input_batch = torch.stack(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: What is the size of the `input_batch` tensor?\n",
    "\n",
    "print(input_batch.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input batch tensor resembles the classic (N, C, H, W) format you will encounter often in your computer vision journey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference on the image\n",
    "Pass the input batch through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: How many elements does `predictions` contain? \n",
    "\n",
    "print(len(predictions))  # must match input batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the number of elements in `prediction` relate to the number of images in the input batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['boxes', 'labels', 'scores'])\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: Explore what each prediction contains. What do you think all these numbers mean?\n",
    "\n",
    "p0 = predictions[0]\n",
    "\n",
    "# print(p0)\n",
    "print(p0.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has returned to us 3 things:\n",
    "- boxes: coordinates of the bounding boxes around detected objects\n",
    "- labels: what it thinks the detected object is \n",
    "- scores: confidence in the predicted label (ranging from 0 - 1, higher is more confident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  3, 10,  8,  3,  1,  1,  1,  1,  3,  1,  1,  1,  3, 10, 10,  6,  3,\n",
       "         1,  6,  3,  1,  3,  1,  8,  1,  1,  1,  3,  1,  1, 10,  6,  3,  8, 10,\n",
       "         3,  3,  1,  1,  3, 10,  3,  8,  3,  8,  3,  1,  3,  1, 10,  3,  6,  1,\n",
       "         8,  1,  1,  1,  3,  6,  6,  1,  8,  3,  3,  1,  3, 10,  1,  1,  1,  1,\n",
       "        31,  6, 10,  1,  3,  6,  1, 33,  3,  1, 10, 41, 27,  1, 14, 31,  8,  8,\n",
       "         3,  8,  6, 31,  6,  2,  1,  3,  1,  6])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXERCISE: See what objects have been detected in the first image\n",
    "\n",
    "p0['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of most-confident prediction:  0.973721981048584\n",
      "Score of least-confident prediction:  0.05608990788459778\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: What are the scores of the most-confident and least-confident predictions?\n",
    "\n",
    "print(\"Score of most-confident prediction: \", max(p0['scores']).item())\n",
    "print(\"Score of least-confident prediction: \", min(p0['scores']).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-process output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has given us integers for labels. These integers are indices that map to object names in the CoCo dataset.\n",
    "\n",
    "Here's a function to load the lookup map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping_dict():\n",
    "    idx_to_labels_url = \"https://gist.githubusercontent.com/suraj813/1fe4c9dd0bc7e1dd1ce79462712ac9ce/raw/0e2c65813946769a375d673a34a1c0236b0505f1/coco_idx_to_labels.txt\"\n",
    "    r = requests.get(idx_to_labels_url).text\n",
    "    map = {int(k) : v for k,v in ast.literal_eval(r).items()}\n",
    "    return map\n",
    "\n",
    "label_lookup = get_mapping_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out! `1` seems to a common label in the first image, what does it correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: What is the object the model predicts as `1`?\n",
    "\n",
    "print(label_lookup[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can translate the model's output labels to actual object names, let's try to build a report for each image.\n",
    "\n",
    "The report should contain all the objects in the image BUT the model isn't confident about every prediction it has made. We should ignore predictions below a certain threshold.\n",
    "\n",
    "There might be multiple occurences of an object in the image; instead of listing every occurrence of the object, the report can just contain an aggregate count of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detection_report(model_output, confidence_threshold=0.8):\n",
    "    bbox, labels, confidence = model_output.values()\n",
    "    labels = labels.tolist()\n",
    "    confidence = confidence.tolist()\n",
    "\n",
    "    label_lookup = get_mapping_dict()\n",
    "    detected_objects = []\n",
    "\n",
    "    for label, confidence in zip(labels, confidence):\n",
    "        if confidence > confidence_threshold:\n",
    "            classname = label_lookup[label]\n",
    "            detected_objects.append((classname, confidence,))\n",
    "    \n",
    "    counts = Counter([x[0] for x in detected_objects])\n",
    "    return detected_objects, counts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects detected in image 1:\n",
      " ====================\n",
      "[('car', 0.973721981048584),\n",
      " ('car', 0.9571205973625183),\n",
      " ('traffic light', 0.9557090401649475),\n",
      " ('truck', 0.9524416327476501),\n",
      " ('car', 0.9516623020172119),\n",
      " ('person', 0.9422258734703064),\n",
      " ('person', 0.9256057739257812),\n",
      " ('person', 0.9153063893318176),\n",
      " ('person', 0.8720185160636902),\n",
      " ('car', 0.8719595074653625),\n",
      " ('person', 0.863501250743866)]\n",
      "\n",
      "Count of objects:\n",
      " ====================\n",
      "Counter({'person': 5, 'car': 4, 'traffic light': 1, 'truck': 1})\n",
      "\n",
      "\n",
      "\n",
      "Objects detected in image 2:\n",
      " ====================\n",
      "[('cat', 0.9777542352676392), ('cat', 0.967362105846405)]\n",
      "\n",
      "Count of objects:\n",
      " ====================\n",
      "Counter({'cat': 2})\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c, pred in enumerate(predictions):\n",
    "    detected_objects, counts = create_detection_report(pred, confidence_threshold=0.85)   \n",
    "\n",
    "    print(f\"Objects detected in image {c+1}:\\n\", \"=\"*20)\n",
    "    pprint(detected_objects)\n",
    "    print()\n",
    "\n",
    "    print(\"Count of objects:\\n\", \"=\"*20)\n",
    "    pprint(counts)\n",
    "    \n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take-home assignment\n",
    "\n",
    "Improve this report by drawing boxes on the input image and labelling each box with the detected object and confidence score.\n",
    "\n",
    "HINT: https://pytorch.org/vision/main/generated/torchvision.utils.draw_bounding_boxes.html"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
