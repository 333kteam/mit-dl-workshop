{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfAtZXG8d3ay"
      },
      "source": [
        "## Summarize audio with natural language processing and automatic speech recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivUIVh_7d3a1"
      },
      "source": [
        "### Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01TgQ5_sd-VY",
        "outputId": "6abaf48e-bbc4-4f29-a0b7-07ebdeea09ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.8/dist-packages (20230124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai-whisper) (1.21.6)\n",
            "Requirement already satisfied: ffmpeg-python==0.2.0 in /usr/local/lib/python3.8/dist-packages (from openai-whisper) (0.2.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from openai-whisper) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.19.0 in /usr/local/lib/python3.8/dist-packages (from openai-whisper) (4.26.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai-whisper) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from openai-whisper) (1.13.1+cu116)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from ffmpeg-python==0.2.0->openai-whisper) (0.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->openai-whisper) (4.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GJnOAtD5d3a2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import torch\n",
        "from typing import List\n",
        "import pandas as pd\n",
        "import whisper\n",
        "from transformers import AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl_Tjd9Vd3a3"
      },
      "source": [
        "### Set options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "278Okd00d3a3"
      },
      "outputs": [],
      "source": [
        "# The size of the ASR model to use\n",
        "ASR_MODEL_SIZE = \"small.en\"\n",
        "\n",
        "# The maximum length of each bullet point (in tokens)\n",
        "SUMMARY_LENGTH = 128\n",
        "\n",
        "# Set device to GPU if available, otherwise use CPU\n",
        "DEVICE = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Play around with other models at https://huggingface.co/models?pipeline_tag=summarization&sort=downloads\n",
        "NLP_ARCH = 'facebook/bart-large-cnn'\n",
        "NLP_TOKENIZER = AutoTokenizer.from_pretrained(NLP_ARCH)\n",
        "NLP_MAXLEN = NLP_TOKENIZER.model_max_length - 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fqLYTwQd3a5"
      },
      "source": [
        "### Download the video and extract audio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1h1C04PDd3a5"
      },
      "outputs": [],
      "source": [
        "# !curl \"https://pytorch-workshops.s3.amazonaws.com/videos/Allen+Newell%2C+%EF%BC%82Desires+and+Diversions%EF%BC%82+%5B405060633%5D.mp4\" -o newell_lecture.mp4\n",
        "# !curl \"https://pytorch-workshops.s3.amazonaws.com/videos/Herb+Simon%2C+%EF%BC%82Intelligence-+Artificial+and+Natural%EF%BC%82+Part+1+%5B395461289%5D.mp4\" -o simon_lecture.mp4\n",
        "# !for name in *.mp4; do ffmpeg -i \"$name\" -vn -acodec libmp3lame -ab 128k \"${name%.*}/audio.mp3\"; done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY7RcmT1d3a6"
      },
      "source": [
        "#### or just download the pre-extracted audio :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoBvmyFfd3a6",
        "outputId": "7fbbe590-ea9f-429e-b5d9-ca4f3948e007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 89.2M  100 89.2M    0     0  22.3M      0  0:00:03  0:00:03 --:--:-- 22.3M\n"
          ]
        }
      ],
      "source": [
        "# create the destination directory first\n",
        "!mkdir newell_lecture\n",
        "# download to it\n",
        "!curl \"https://pytorch-workshops.s3.amazonaws.com/videos/audio/Allen+Newell%2C+%EF%BC%82Desires+and+Diversions%EF%BC%82+%5B405060633%5D.mp3\" -o newell_lecture/audio.mp3\n",
        "# !mkdir simon_lecture\n",
        "# !curl \"https://pytorch-workshops.s3.amazonaws.com/videos/audio/Herb+Simon%2C+%EF%BC%82Intelligence-+Artificial+and+Natural%EF%BC%82+Part+1+%5B395461289%5D.mp3\" -o simon_lecture/audio.mp3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPC3ADJTd3a6"
      },
      "source": [
        "### Choose a lecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kub7v2wMd3a7"
      },
      "outputs": [],
      "source": [
        "audio_file = \"newell_lecture/audio.mp3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU9dQa47d3a7"
      },
      "source": [
        "### Load the Whisper ASR Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OyFBI0-kd3a7"
      },
      "outputs": [],
      "source": [
        "asr_model = whisper.load_model(ASR_MODEL_SIZE).to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYeLojmBd3a7"
      },
      "source": [
        "### Transcribe speech in the audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "POIffG0fd3a7"
      },
      "outputs": [],
      "source": [
        "# EXERCISE:\n",
        "# Write a function to \n",
        "# - transcribe an audio file using an ASR model \n",
        "# - save the resulting transcription as a CSV file\n",
        "# - return the file path of the resulting transcript.\n",
        "#\n",
        "# You may use the helper function `save_transcript_to_csv` to save Whisper's output as a csv file\n",
        "\n",
        "\n",
        "def save_transcript_to_csv(asr_result, file_path):\n",
        "    \"\"\"\n",
        "    Save transcription to a CSV file\n",
        "    \n",
        "    Parameters:\n",
        "        asr_result: The transcription data to save.\n",
        "        file_path (str): The file path of the CSV file.\n",
        "    \"\"\"\n",
        "    field_names = ['start', 'end', 'text', 'summary']\n",
        "    with open(file_path, \"w\", newline=\"\") as csv_file:\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=field_names)\n",
        "        writer.writeheader()\n",
        "        for entry in asr_result:\n",
        "            writer.writerow({k:entry[k] for k in field_names if k in entry.keys()})\n",
        "\n",
        "\n",
        "def transcribe_audio(audio_path: str, asr_model):\n",
        "    \"\"\"\n",
        "    Transcribe an audio file using the provided ASR model.\n",
        "    \n",
        "    Parameters:\n",
        "        audio_path (str): The file path of the audio file.\n",
        "        asr_model: The ASR model to use for transcription.\n",
        "    \n",
        "    Returns:\n",
        "        str: The file path of the transcript.\n",
        "    \"\"\"\n",
        "    dir_name = os.path.dirname(audio_path)\n",
        "    transcript_path = f\"{dir_name}/transcript.csv\"\n",
        "\n",
        "    # Save the audio transcript as a csv file at the transcript filepath\n",
        "    if not os.path.exists(transcript_path):        \n",
        "        with torch.no_grad():\n",
        "            asr_result = asr_model.transcribe(audio_path)['segments']\n",
        "        save_transcript_to_csv(asr_result, transcript_path)\n",
        "        \n",
        "    return transcript_path    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ad5Vm5-Od3a7"
      },
      "outputs": [],
      "source": [
        "# this takes a while... \n",
        "transcript_path = transcribe_audio(audio_file, asr_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3mtTTItd3a8"
      },
      "source": [
        "### Preview the transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ykmhGD2_d3a8",
        "outputId": "5b6a3445-173b-4cd9-ea1d-9819914bb81e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   start   end                                               text  summary\n",
              "0    0.0  23.0   So I could just hear people talking coming up...      NaN\n",
              "1   23.0  31.0   When you get to be two to the sixth years, yo...      NaN\n",
              "2   31.0  33.0              so you've got to talk about the past.      NaN\n",
              "3   33.0  37.0                 But that's not true, it turns out.      NaN\n",
              "4   37.0  41.0   And I could have talked to you about SOAR, al...      NaN"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c4b61852-b34f-4098-96ce-c6c2cf37d3b4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>So I could just hear people talking coming up...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>When you get to be two to the sixth years, yo...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>so you've got to talk about the past.</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>But that's not true, it turns out.</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>37.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>And I could have talked to you about SOAR, al...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4b61852-b34f-4098-96ce-c6c2cf37d3b4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c4b61852-b34f-4098-96ce-c6c2cf37d3b4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c4b61852-b34f-4098-96ce-c6c2cf37d3b4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "transcript_df = pd.read_csv(transcript_path, header=0)\n",
        "transcript_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTMLMFlmd3a8"
      },
      "source": [
        "### Generate summary of transcription\n",
        "\n",
        "We have the raw transcript in `transcript.csv`. The csv file has 4 columns: start, end, text, summary. So far the first three columns have been populated by the ASR model. Let's populate the summaries now.\n",
        "\n",
        "One approach might be to generate a summary for each row. But as you can see, each row corresponds to around 7 seconds of the lecture, which also includes silence. So some rows have barely any words at all - not a good candidate for summarization!\n",
        "\n",
        "NLP models have a maximum input length they can accept; in case of our model it is 1024. We'll iterate over the `text` column and chunk it up into segments containing 1019 tokens or less. (we use a buffer of 5 tokens to prevent inadvertent overflows).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens(tokenizer, input_text):\n",
        "    return tokenizer(input_text, add_special_tokens=False)['input_ids']\n",
        "\n",
        "\n",
        "def generate_timestamped_segments(asr_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Tokenize transcribed text, chunking into segments of length <= NLP_MAXLEN-5\n",
        "    while preserving correct timestamps from ASR transcription.\n",
        "    \n",
        "    Parameters:\n",
        "        asr_result (List[dict]): The transcription segments. Each dict should have a 'text' and \n",
        "            'start' and 'end' keys for the transcription text and start and end times, respectively.\n",
        "    \n",
        "    Returns:\n",
        "        List[dict]: A list of dictionaries containing tokenized sentences with 'text', 'tokens', 'start',\n",
        "            and 'end' keys for the sentence text, tokenized form, and start and end times, respectively.\n",
        "    \"\"\"\n",
        "    segments = []\n",
        "    curr_segment = {\n",
        "        \"start\": None,\n",
        "        \"end\": None,\n",
        "        \"text\": \"\",\n",
        "        \"tokens\": []\n",
        "    }\n",
        "\n",
        "    for _, row in asr_df.iterrows():\n",
        "        text = row['text']\n",
        "        tokens = get_tokens(NLP_TOKENIZER, text)\n",
        "        # If total tokens exceed maximum length, add current segment to list and flush\n",
        "        if len(tokens) + len(curr_segment['tokens']) >= NLP_MAXLEN:\n",
        "            segments.append(curr_segment)\n",
        "            curr_segment = {\n",
        "                \"start\": None,\n",
        "                \"end\": None,\n",
        "                \"text\": \"\",\n",
        "                \"tokens\": []\n",
        "            }\n",
        "        else:\n",
        "            curr_segment['start'] = curr_segment['start'] or row['start']\n",
        "            curr_segment['end'] = row['end']\n",
        "            curr_segment['text'] += text \n",
        "            curr_segment['tokens'] += tokens\n",
        "\n",
        "    segments.append(curr_segment)\n",
        "    return pd.DataFrame(segments)"
      ],
      "metadata": {
        "id": "hRQLg_nxoaox"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segments_df = generate_timestamped_segments(transcript_df)"
      ],
      "metadata": {
        "id": "b4v65daFodN9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore the generated dataframe. Compared to `transcript_df`, what is the time duration that each row captures? How many words on average in each segment?"
      ],
      "metadata": {
        "id": "JkCLFKVBpgeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE: what is the time duration that each row captures? \n",
        "print((segments_df['end'] - segments_df['start']).mean(), \"seconds\")\n",
        "\n",
        "# EXERCISE: How many words on average in each segment?\n",
        "print(segments_df[\"text\"].str.len().mean())\n",
        "\n",
        "# EXERCISE: How many tokenson average in each segment?\n",
        "print(segments_df[\"tokens\"].str.len().mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeEX5hxNpX9E",
        "outputId": "f91fa658-f1b3-4c06-fdc7-4df63b80dcfd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "302.57894736842104 seconds\n",
            "4334.315789473684\n",
            "961.8421052631579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify that no text segment contains more than NLP_MAXLEN tokens"
      ],
      "metadata": {
        "id": "jHEa20XMxJVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert (segments_df['tokens'].str.len() > NLP_MAXLEN).sum() == 0, f\"At least one text segment has more than {NLP_MAXLEN} tokens\""
      ],
      "metadata": {
        "id": "4q8ILTKk6Jap"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to pass these text segments to an NLP Summarizer model. For each text segment the model will generate summaries that are not more than `SUMMARY_LENGTH = 128` tokens.\n",
        "\n",
        "The `transformers` library offers a convenient `pipeline` class that wraps up complex code for summarization (and more tasks) into a single API call. [API Reference](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/pipelines#transformers.SummarizationPipeline)\n",
        "\n",
        "We defined `NLP_ARCH = facebook/bart-large-cnn` above; this is Meta's BART language model that has been finetuned for summarization on the CNN/Daily Mail dataset."
      ],
      "metadata": {
        "id": "Zwa4Jgihu05f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOLfQLlqBL2Q",
        "outputId": "68868bf0-fccd-499f-8cff-24737da18f84"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bart_abstract_1 = \"BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token.\"\n",
        "bart_abstract_2 = \" BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.\"\n",
        "bart_abstracts = [bart_abstract_1, bart_abstract_2]\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=NLP_ARCH, device=torch.device('cuda:0'))\n",
        "abstracts_summary = summarizer(bart_abstracts, max_length=64, min_length=20)\n",
        "\n",
        "print(abstracts_summary[0])\n",
        "print(abstracts_summary[1])\n",
        "\n",
        "print(\"original length: \", len(bart_abstract_1) + len(bart_abstract_2))\n",
        "print(\"summarized length: \", len(abstracts_summary[0]['summary_text']) + len(abstracts_summary[1]['summary_text']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKaaQExk_FK7",
        "outputId": "1406a188-9f3c-4ab8-89e7-62e0223d16e5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'summary_text': 'BART is a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by corrupting text with an arbitrary noising function. It uses a standard Tranformer-based neural machine translation architecture.'}\n",
            "{'summary_text': 'BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD. BART achieves new state-of-the-art results on a range of abstractive dialogue, question answering,'}\n",
            "original length:  1361\n",
            "summarized length:  526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPeu--k7d3a8",
        "outputId": "0475bf19-b7bb-43fe-ceac-3e6c8cf2156e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 128, but you input_length is only 59. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n"
          ]
        }
      ],
      "source": [
        "# EXERCISE: \n",
        "# Write a function that \n",
        "# - takes in the dataframe generated above, \n",
        "# - generates summaries of 128 tokens or less for each segment, and \n",
        "# - adds them in a new column of the dataframe. Return this dataframe\n",
        "\n",
        "def generate_timestamped_summaries(segments_df: pd.DataFrame, summary_lengths: int = 128):\n",
        "    \"\"\"\n",
        "    Generate summaries of each timestamped segments\n",
        "    \n",
        "    Parameters:\n",
        "        segments_df (DataFrame): The dataframe containing timestamps and text segments\n",
        "        summary_lengths (int): The maximum length of each generated summary\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: A dataframe with timestamps, transcriptions and summaries\n",
        "    \"\"\"\n",
        "       \n",
        "    # Extract the sentences from the timestamped transcript\n",
        "    sentences = segments_df['text'].tolist()\n",
        "    \n",
        "    # Initialize the summarization pipeline\n",
        "    summarizer = pipeline(\"summarization\", model=NLP_ARCH, device=DEVICE)\n",
        "    \n",
        "    # Generate summaries for the sentences\n",
        "    summaries = summarizer(sentences, max_length=summary_lengths, min_length=20, do_sample=False)\n",
        "    summaries = [x['summary_text'] for x in summaries]\n",
        "    \n",
        "    # Add the summaries to the dataframe\n",
        "    segments_df['summary'] = summaries\n",
        "    # for a, b in zip(timestamped_sentences, summaries):\n",
        "    #     a['summary'] = b['summary_text']\n",
        "    \n",
        "    # Save the timestamped summaries to a file\n",
        "    # summary_file = os.path.dirname(transcript_file)+\"/summary.csv\"\n",
        "    # save_transcript_to_csv(timestamped_sentences, summary_file)\n",
        "    # print(\"[NLP] Video summary saved at \", summary_file)\n",
        "\n",
        "    return segments_df\n",
        "\n",
        "summary_df = generate_timestamped_summaries(segments_df, SUMMARY_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fyms064Ud3a8"
      },
      "source": [
        "### View the summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "segments_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "179Ds1LVBb0l",
        "outputId": "b7b97e1c-9a21-4178-976e-97e54d4c4820"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    start     end                                               text  \\\n",
              "0    23.0   318.0   So I could just hear people talking coming up...   \n",
              "1   322.0   622.0   and it's very hard to prove theorems that are...   \n",
              "2   626.0   919.0   That is the AI question. And although I have ...   \n",
              "3   922.0  1226.0   Who would ever want to be an optical engineer...   \n",
              "4  1229.0  1532.0   And I read this chapter, and it was so awful ...   \n",
              "\n",
              "                                              tokens  \\\n",
              "0  [407, 38, 115, 95, 1798, 82, 1686, 567, 62, 11...   \n",
              "1  [8, 24, 18, 182, 543, 7, 3364, 5, 1688, 4339, ...   \n",
              "2  [280, 16, 5, 4687, 864, 4, 178, 1712, 38, 33, ...   \n",
              "3  [3394, 74, 655, 236, 7, 28, 41, 17547, 8083, 1...   \n",
              "4  [178, 38, 1166, 42, 7285, 6, 8, 24, 21, 98, 11...   \n",
              "\n",
              "                                             summary  \n",
              "0  Soar is built out of a big production system, ...  \n",
              "1  In complexity theory, someone proves a really ...  \n",
              "2  At age 17 he wanted to be a forest ranger. At ...  \n",
              "3  As an undergraduate at Stanford, I worked on x...  \n",
              "4  In 1970, when Xerox came into existence in 197...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1aefd43b-cc84-482a-823b-7bdfe225a182\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>text</th>\n",
              "      <th>tokens</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23.0</td>\n",
              "      <td>318.0</td>\n",
              "      <td>So I could just hear people talking coming up...</td>\n",
              "      <td>[407, 38, 115, 95, 1798, 82, 1686, 567, 62, 11...</td>\n",
              "      <td>Soar is built out of a big production system, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>322.0</td>\n",
              "      <td>622.0</td>\n",
              "      <td>and it's very hard to prove theorems that are...</td>\n",
              "      <td>[8, 24, 18, 182, 543, 7, 3364, 5, 1688, 4339, ...</td>\n",
              "      <td>In complexity theory, someone proves a really ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>626.0</td>\n",
              "      <td>919.0</td>\n",
              "      <td>That is the AI question. And although I have ...</td>\n",
              "      <td>[280, 16, 5, 4687, 864, 4, 178, 1712, 38, 33, ...</td>\n",
              "      <td>At age 17 he wanted to be a forest ranger. At ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>922.0</td>\n",
              "      <td>1226.0</td>\n",
              "      <td>Who would ever want to be an optical engineer...</td>\n",
              "      <td>[3394, 74, 655, 236, 7, 28, 41, 17547, 8083, 1...</td>\n",
              "      <td>As an undergraduate at Stanford, I worked on x...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1229.0</td>\n",
              "      <td>1532.0</td>\n",
              "      <td>And I read this chapter, and it was so awful ...</td>\n",
              "      <td>[178, 38, 1166, 42, 7285, 6, 8, 24, 21, 98, 11...</td>\n",
              "      <td>In 1970, when Xerox came into existence in 197...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1aefd43b-cc84-482a-823b-7bdfe225a182')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1aefd43b-cc84-482a-823b-7bdfe225a182 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1aefd43b-cc84-482a-823b-7bdfe225a182');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format and save the dataframe\n",
        "\n",
        "Looks good! Let's make the timestamps more readable so we can scroll in the video if we need to. Here's a `format_time` helper function.\n",
        "\n",
        "Save the dataframe as a CSV file. This will be useful when we need to cross-check something in the video"
      ],
      "metadata": {
        "id": "an0BHx_37T3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_time(t):\n",
        "    \"\"\"\n",
        "    Convert a time in seconds to a string in HH:MM:SS format.\n",
        "    \n",
        "    Parameters:\n",
        "        t (str): The time in seconds.\n",
        "    \n",
        "    Returns:\n",
        "        str: The time in HH:MM:SS format.\n",
        "    \"\"\"\n",
        "    t = round(float(t))\n",
        "    hh = t // 3600\n",
        "    t %= 3600\n",
        "    mm = t // 60\n",
        "    ss = t % 60\n",
        "    return f\"{hh:02d}:{mm:02d}:{ss:02d}\"\n",
        "\n",
        "\n",
        "segments_df['start'] = segments_df['start'].apply(format_time)\n",
        "segments_df['end'] = segments_df['end'].apply(format_time)\n",
        "\n",
        "output_dir = os.path.dirname(audio_file)\n",
        "segments_df.to_csv(output_dir + '/timestamped_summaries.csv', index_label=False)"
      ],
      "metadata": {
        "id": "RJiOjEA37Vxl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the summary row-by-row is tedious. Concatenate all the individial summaries into a single passage."
      ],
      "metadata": {
        "id": "lrEVEQumCVda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE: Concatenate the segment-summaries into a single paragraph\n",
        "\n",
        "passage = \" \".join(segments_df['summary'].tolist())\n",
        "print(passage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CdV4eSrCKGQ",
        "outputId": "fb917d39-2da7-4ecf-fb22-ba83b8678f8b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Soar is built out of a big production system, and thus there is behind SOAR a huge set of rules, 10,000 of them by the time you take the 2,000, original and 8,000. There's a big read-a-net there, which is a device for trying to execute those rules efficiently. So here we have the time per decision cycle in seconds, spin it on a log scale, and in fact, there's essentially no indication of what we call the average growth effect. In complexity theory, someone proves a really new interesting theorem which opens up a new area. You live your life moving from one theoretical area to another as it opens up. For Werner Reichert, life is a sequence of five years, phenomenally five years project, each one picked by looking at the state of science. At age 17 he wanted to be a forest ranger. At 19 he was out at Bikini at the atomic bomb test site. At 22 he was an optical engineer. At 25 he was a computer scientist. At 30 he became a computer engineer. As an undergraduate at Stanford, I worked on x-ray microscopy. I clearly wanted to be an optical engineer. It hadn't happened yet at age 27 where what I wanted was an organizational scientist. Gordon Bell came to town and got tired of building machines, so he showed up at MIT. And he was writing a book about his experience in building machines. In 1970, when Xerox came into existence in 1970, we generated a consultancy out there. We invented the notion that maybe it was time to apply all that we understood about cognition to computers. So we generated that consultancy, and that was the diversion, and so we generated the Xerox PART. And then, of course, there was Raj Reddy and the speech understanding system. In 1975, 1975, 76, we sort of developed strongly the notion of routine cognitive skills. We devoted a class of models called GOMS for characterizing when humans are operating in a routine fashion in a cognitive skill. We helped establish the whole field of HCI as an endeavor. The maxim here is embrace failure as part of success. I always like to say, you know, one good failure a week is just bracing and good for you. So now I want to tell you about a couple of failures. Here's the first one. It's a system called Merlin, which I did with Jim. Moore. The Instructable Production System Project lasted from 76 to 79. It was in fact a total failure. We never got off the ground. But it turns out that it had an incredible set of uses. Bob Greene: List processing is one of the big successes of my career. Greene: You can't be seduced into devoting your life to list processing since it sets a success. He says protocol analysis, introspection and Tashing were also big successes. Greene says he did not follow up on cognitive psychology and science. When you make a success, when you get an insight into the essential problem that you're working on, you'll preserve it. You work on it and deepen it. Any really deep idea is going to transform itself radically over time. The problem space is the notion that relates to how the human limits the arena. By 1979, this notion had gotten enriched because it wasn't just problem solving. In 1990, which is sort of a... This is actually a little odd in time, but I'll just let it go. SoAR is not a final project. It's simply the next project that comes along. Science is genuinely the art of the possible. All of them, if I take one, are I could view it each worth five years, all right? So clearly I don't have a chance. Someplace down in here, it's all over. If you want to work with the results of field X, you've got to be a professional in X to wit no interdisciplinary activity. In cognitive psychology, which was central for him, is central for me in understanding the nature of human cognition. If you can't talk that way, if it turns out that you don't know, essentially every study that's laid on the conversational table, you are not a cognitive psychologist. Scientist says science is in the technique, all else is commentary. He says scientists should listen to nature to find out what it knows about human behavior. Scientist says he has never organized scientific meetings or been on a program committee. He says he was a graduate student at Pine Hall, where he worked on Dean Theory of RAM. \"I've got a lot of great stories to tell\" Scientist says he spends 30 hours a week on research and 50 on other things. He says he's proud of his work and the way it's been developed. But he says he has to deal with the diversions in his life. I decided to be an optical engineer with a passion. And so in fact maybe I was just a set-up waiting to be in print. It turned out Oliver got there first. But I never did somehow decide to do that as a means to an end. I got committed because that happened to me. I consider that I have done a hell of a lot of computer science. My goal is therefore not computer science, just like my goal is not AI. That doesn't keep me from being in the computer science department, providing textbooks on computers. A third of your resources implementing SOAR and C, but that turned out to be the thing to do, so you go do that. And so you just sort of play that one in real random order and hope that the funding people never catch up with you. Thank you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're busy people, we need a TL;DR"
      ],
      "metadata": {
        "id": "Co0EGSFpGd4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE: Generate a synopsis of the entire video not exceeding 64 tokens \n",
        "\n",
        "tokens = get_tokens(NLP_TOKENIZER, passage)\n",
        "summarizer = pipeline(\"summarization\", model=NLP_ARCH, device=DEVICE)\n",
        "\n",
        "if len(tokens) > NLP_MAXLEN:\n",
        "  tokens_list = [tokens[i:i+NLP_MAXLEN] for i in range(0, len(tokens), NLP_MAXLEN)]\n",
        "  sentence_list = [NLP_TOKENIZER.decode(toks) for toks in tokens_list]\n",
        "  output = summarizer(sentence_list, max_length=64, min_length=20, do_sample=False)\n",
        "else:\n",
        "  output = summarizer(passage, max_length=64, min_length=20, do_sample=False)\n",
        "\n",
        "synopsis = ' '.join([x['summary_text'] for x in output])\n",
        "print(synopsis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t0SXy8jGRZ3",
        "outputId": "2816bf20-2fb2-46d9-f956-6de3d1cc5647"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Werner Reichert: Life is a sequence of five years, phenomenally five years project, each one picked by looking at the state of science. At age 17 he wanted to be a forest ranger. At 19 he was out at Bikini at the atomic bomb test site. At 22 I consider that I have done a hell of a lot of computer science. My goal is therefore not computer science, just like my goal is not AI. That doesn't keep me from being in the computer science department, providing textbooks on computers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the TL;DR and paragraph to a file"
      ],
      "metadata": {
        "id": "B7DmuyIFHChm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE: Write a file called video_summary.txt that contains the above generated synopsis and passage\n",
        "\n",
        "with open(output_dir + '/video_summary.txt', \"w\") as f:\n",
        "  f.write(\"TL;DR\\n\")\n",
        "  f.write(synopsis)\n",
        "  f.write(\"\\n\\nPASSAGE:\\n\")\n",
        "  f.write(passage)\n",
        "\n"
      ],
      "metadata": {
        "id": "0rWT0i2uHHaE"
      },
      "execution_count": 13,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
    },
    "kernelspec": {
      "display_name": "Python 3.9.11 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}