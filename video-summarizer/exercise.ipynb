{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfAtZXG8d3ay"
      },
      "source": [
        "## GistTube\n",
        "\n",
        "This workshop demonstrates how to create an AI/ML pipeline to convert spoken audio into a text summary. You will learn how to convert audio into text via automatic speech recognition and natural language processing using the Whisper ASR model.\n",
        "\n",
        "Upon completion you will have a basic understanding of:\n",
        "\n",
        "1. FFMPEG for audio extraction\n",
        "1. Automatic speech recognition \n",
        "1. NLP concepts (tokenization, summarization)\n",
        "1. Whisper and Huggingface APIs\n",
        "1. Pandas DataFrames\n",
        "\n",
        "\n",
        "**Note:** This file is intended to be run on [Google Colab](https://colab.research.google.com). If you're viewing this file on github, [click here](https://githubtocolab.com/fbsamples/mit-dl-workshop/blob/main/video-summarizer/exercise.ipynb) to load it into google colab.\n",
        "\n",
        "If you get stuck, feel free to refer to the [solution](https://githubtocolab.com/fbsamples/mit-dl-workshop/blob/main/video-summarizer/solution.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivUIVh_7d3a1"
      },
      "source": [
        "### 1. Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01TgQ5_sd-VY",
        "outputId": "6abaf48e-bbc4-4f29-a0b7-07ebdeea09ca"
      },
      "outputs": [],
      "source": [
        "%pip install -U openai-whisper "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJnOAtD5d3a2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import whisper\n",
        "from transformers import AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl_Tjd9Vd3a3"
      },
      "source": [
        "### 2. Set options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "278Okd00d3a3"
      },
      "outputs": [],
      "source": [
        "# The size of the ASR model to use\n",
        "ASR_MODEL_SIZE = \"small.en\"\n",
        "\n",
        "# The maximum length of each bullet point (in tokens)\n",
        "SUMMARY_LENGTH = 128\n",
        "\n",
        "# Set device to GPU if available, otherwise use CPU\n",
        "DEVICE = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# This is the NLP model for summarization\n",
        "# Play around with other models at https://huggingface.co/models?pipeline_tag=summarization&sort=downloads\n",
        "NLP_ARCH = 'facebook/bart-large-cnn'\n",
        "\n",
        "# We will save any artifact we create in this folder\n",
        "FOLDER = \"refik_interview\"\n",
        "os.makedirs(FOLDER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fqLYTwQd3a5"
      },
      "source": [
        "### 3. Download the video and extract audio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this step you will download the video that you will be summarizing. Later you will use the Whisper ASR model to transcribe the audio. It only accepts audio as input, so you will need to extract the audio from the video file. \n",
        "\n",
        "In this step of the excercise you will:\n",
        "\n",
        "1. Download the video file.\n",
        "1. Extract the audio. \n",
        "\n",
        "To download the file from the Amazon S3 bucket we provided, you will use the curl command. It's a tool for transferring data from or to a server. Then you will use FFMPEG command to extract the audio. \n",
        "\n",
        "*FFMPEG is a suite of libraries and programs for handling video, audio, other multimedia files, and streams.*\n",
        "\n",
        "Now that you understand the process, run the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h1C04PDd3a5"
      },
      "outputs": [],
      "source": [
        "# these are shell commands, but you can run them from a jupyter notebook (like colab) by prefixing an exclamation mark (!)\n",
        "\n",
        "!curl \"https://pytorch-workshops.s3.amazonaws.com/videos/refik_interview.mp4\" -o refik_interview/video.mp4\n",
        "!ffmpeg -i refik_interview/video.mp4 -vn -acodec libmp3lame -ab 128k refik_interview/audio.mp3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üèÜüèÜ Congrats! With just two lines of code you have downloaded a video off the internet and extracted the audio into a separate file! Pretty neat huh! üòÄ üèÜüèÜ\n",
        "\n",
        "If you recall above we mentioned that the Whisper ASR model requires audio as input. Since our source material was a video we had to extract the audio. \n",
        "\n",
        "Lets say our source material was an audio podcast. \n",
        "\n",
        "**Question:** Would we need to use ffmpeg to extract the audio?\n",
        "\n",
        "**Answer:** No! In that case we wouldn't need to extract the audio because the file is already in the correct format - audio!\n",
        "\n",
        "---- \n",
        "**Note:** For this workshop we included the audio-only version of the file in our Amazon S3 bucket. To download just the **audio/refik_interview.mp3** file, you would run the code below:\n",
        "\n",
        "`!curl \"https://pytorch-workshops.s3.amazonaws.com/videos/audio/refik_interview.mp3\" -o refik_interview/audio.mp3`\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that you have your audio file to transcribe, you need to build the ASR model that will do the transcription. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU9dQa47d3a7"
      },
      "source": [
        "### 4. Load the Whisper ASR Model\n",
        "\n",
        "For this workshop, you will use the Whisper ASR model ([blog](https://openai.com/blog/whisper/)). It is one of the best performing ASR models today. \n",
        "\n",
        "Here are the instructions for using Whisper ASR in python: https://github.com/openai/whisper#python-usage\n",
        "\n",
        "Run the code below to load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyFBI0-kd3a7"
      },
      "outputs": [],
      "source": [
        "asr_model = whisper.load_model(ASR_MODEL_SIZE).to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üèÜüèÜ Now that the model was loaded you can transcibe the textüèÜüèÜ\n",
        "\n",
        "In the next step you will test out the code for transcription on a 3-second audio file we provided for you. \n",
        "\n",
        "Go ahead and open the file in your browser to listen to it: [Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav](https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav)\n",
        "\n",
        "Now that you've given it a listen, it's time to transcribe it! To do that use the `.transcribe` function:\n",
        "\n",
        "`asr_model.transcribe(\"<path to file>\")`\n",
        "\n",
        "The code below has already been written to download and save the audio into a file called **tmp_audio.wmv**. \n",
        "\n",
        "Calling transcribe will return a dictionary that includes some information about the file in addition to the transcribed text.\n",
        "\n",
        "Given that information find and replace `# write code here` with the code to transcribe the model and print the transcribed text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCISE: \n",
        "# - Test the ASR model on an audio file at https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\n",
        "# - What does result['segments'] return?\n",
        "\n",
        "# download the audio file\n",
        "import requests\n",
        "with requests.get(\"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\", stream=True) as resp:\n",
        "    with open('tmp_audio.wav', 'wb') as f:\n",
        "        f.write(resp.content)\n",
        "\n",
        "\n",
        "# Run ASR transcription\n",
        "# ... \n",
        "# write code here\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üèÜüèÜWere you able to transcribe the text? Does it match what the audio file says? If so, congratulations!üèÜüèÜ\n",
        "\n",
        "Now it's time to learn a little bit more about how the transcribe function handles the audio for larger files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYeLojmBd3a7"
      },
      "source": [
        "### 5. Transcribe speech in the audio\n",
        "\n",
        "The ASR model chunks up the audio into \"segments\". Since the audio above is short, it has only one segment; longer audio tracks will have more segments. \n",
        "\n",
        "Each segment has a start and end timestamp, along with the transcribed text. We'll load all the segments into a pandas dataframe. DataFrames are like spreadsheets in python; they make data easier to read and manage. In our dataframe, each row corresponds to one segment. \n",
        "\n",
        "For this excercise you will write a function that takes an audio path (`audio_path`) and an ASR model (`asr_model`), and returns the transcribed dataframe `transcript_df`. To accomplish this task you will need to:\n",
        "\n",
        "1. Transcribe `audio_path` using the `.transcribe` function you learned above.\n",
        "1. Create the dataframe and load the segments into the pandas dataframe: `pd.DataFrame(result[\"segments\"])`\n",
        "1. Keep only `['start', 'end', 'text']` columns.\n",
        "\n",
        "\n",
        "The code to save the dataframe to a CSV has been provided for you. Make sure to name your pandas dataframe `transcript_df`.\n",
        "\n",
        "Now that you understand what you need to do, try it yourself. If you get stuck, check out the [solution](https://githubtocolab.com/fbsamples/mit-dl-workshop/blob/main/video-summarizer/solution.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POIffG0fd3a7"
      },
      "outputs": [],
      "source": [
        "# EXERCISE:\n",
        "# Write a function to \n",
        "# - transcribe an audio file using an ASR model \n",
        "# - load the start, end, text values of the ASR result into a pandas dataframe\n",
        "# - save the dataframe as a csv file in {FOLDER}\n",
        "# - return the dataframe\n",
        "\n",
        "\n",
        "def transcribe_audio(audio_path: str, asr_model):\n",
        "    \"\"\"\n",
        "    Transcribe an audio file using the provided ASR model.\n",
        "    \n",
        "    Parameters:\n",
        "        audio_path (str): The file path of the audio file.\n",
        "        asr_model: The ASR model to use for transcription.\n",
        "    \n",
        "    Returns:\n",
        "        str: The file path of the transcript.\n",
        "    \"\"\"\n",
        "    # ... \n",
        "    # write code here\n",
        "    # ...\n",
        "    \n",
        "    transcript_df.to_csv(f\"{FOLDER}/transcript.csv\", index_label=False)\n",
        "    return transcript_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.1 Test your function\n",
        "\n",
        "Once you've written the function, try it on the 3 second audio you just downloaded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transcribe_audio('tmp_audio.wav', asr_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üèÜüèÜ Do you see a dataframe with a single row and 3 columns? If so, good job! üèÜüèÜ\n",
        "\n",
        "If you're stuck don't worry. We've got you covered. Compare your code with the [solution](https://github.com/fbsamples/mit-dl-workshop/blob/main/video-summarizer/solution.ipynb).\n",
        "\n",
        "Now that your function works, it's time to transcribe the full audio file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.2 Transcribe the full audio file\n",
        "\n",
        "The time has finally come to transcribe the audio file you downloaded into text! The file is quite large so it will take a while to transcribe. Feel free to grab a drink, get some coffee, and put on your favorte cat videos while you wait üò∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad5Vm5-Od3a7"
      },
      "outputs": [],
      "source": [
        "audio_file = f\"{FOLDER}/audio.mp3\"\n",
        "\n",
        "transcript_df = transcribe_audio(audio_file, asr_model)  # this takes a while... \n",
        "transcript_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üèÜüèÜCongratulations! You now have fully transcribed the audio file containing multiple segments!üèÜüèÜ\n",
        "\n",
        "In the next step you will convert that transcript into a summary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTMLMFlmd3a8"
      },
      "source": [
        "### 5. Generate summary of transcription\n",
        "\n",
        "The `transcribe_audio` function you wrote saved the raw transcript in `transcript.csv`. The csv file has 3 columns: start, end, text, populated by the ASR model. It's now time to create the next part of the project that will populate the summary column.\n",
        "\n",
        "Given that one row represents one segement, you could generate a summary for each row. But consider how each segment is composed. Each row corresponds to around 7 seconds of audio, which includes silence. Some rows have barely any words at all - this is not a good candidate for summarization!\n",
        "\n",
        "NLP models have a maximum input length they can accept; in case of our model it is 1024. We'll iterate over the `text` column and chunk it up into segments containing 1019 tokens or just under that. \n",
        "\n",
        "**Question:** Why split segments into 1019 instead of 1024 tokens?\n",
        "\n",
        "**Answer:** To prevent inadvertent overflows we chose to use a buffer of 5 tokens - 1024 - 5 = 1019.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5.1 NLP Tokenization\n",
        "\n",
        "We defined `NLP_ARCH = facebook/bart-large-cnn` above; this is Meta's BART language model that has been finetuned for summarization on the CNN/Daily Mail dataset. We need to use the tokenizer for this architecture. \n",
        "\n",
        "The `transformers` library has an `AutoTokenizer` class that provides model-specific tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NLP_TOKENIZER = AutoTokenizer.from_pretrained(NLP_ARCH)\n",
        "\n",
        "def get_tokens(tokenizer, input_text):\n",
        "    return tokenizer(input_text, add_special_tokens=False)['input_ids']\n",
        "\n",
        "text = \"Artifical Intelligence brings forth a new dawn of the industrial age\"\n",
        "tokens = get_tokens(text)\n",
        "\n",
        "print(\"Number of words: \", len(' '.split(text)))\n",
        "print(\"Number of tokens: \", len(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üèÜüèÜ Great! Now that you've got a hang of it, we're going to step back and challenge you to continue down this notebook. üèÜüèÜ\n",
        "\n",
        "Of course, feel free to ask your instructors if you have any questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5.2 Tokenize the transcribed text and create longer segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRQLg_nxoaox"
      },
      "outputs": [],
      "source": [
        "\n",
        "NLP_MAXLEN = NLP_TOKENIZER.model_max_length - 5\n",
        "\n",
        "def generate_timestamped_segments(asr_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Tokenize transcribed text, chunking into segments of length <= NLP_MAXLEN-5\n",
        "    while preserving correct timestamps from ASR transcription.\n",
        "    \n",
        "    Parameters:\n",
        "        asr_df (pd.DataFrame): The transcription segments in dataframe format. \n",
        "        Contains columns 'text', 'start', 'end'.\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: Dataframe where each row is a segment. Must contain columns 'text', 'start', 'end', and\n",
        "        'tokens' corresponding to the NLP tokens of 'text'\n",
        "    \"\"\"\n",
        "    # ...\n",
        "    # write code here\n",
        "    # ...\n",
        "\n",
        "    # HINTS:\n",
        "    # - Create an empty list called segments.\n",
        "    # - Create a dictionary called curr_segment with keys 'start', 'end', 'text', and 'tokens'. \n",
        "    #   Each value is initialized to None, \"\" or [].\n",
        "    # - Iterate through each row in asr_df.\n",
        "    # - Get the value of 'text' in the current row and assign it to a variable called text.\n",
        "    # - Tokenize the text using the function get_tokens and store the result in a variable called tokens.\n",
        "    # - If the total number of tokens in the current segment plus the number of tokens in the tokens list \n",
        "    #   exceed the maximum length, add the current segment to segments list and create a new curr_segment.\n",
        "    # - If the total number of tokens in the current segment plus the number of tokens in the tokens list \n",
        "    #   does not exceed the maximum length, update the curr_segment dictionary with the 'start', 'end', \n",
        "    #   'text' and 'tokens' values.\n",
        "    # - Append the current curr_segment to the segments list.\n",
        "    # - Create a pandas DataFrame from the segments list.\n",
        "    # - Return the pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4v65daFodN9"
      },
      "outputs": [],
      "source": [
        "segments_df = generate_timestamped_segments(transcript_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkCLFKVBpgeI"
      },
      "source": [
        "#### 5.3 Explore the generated dataframe \n",
        "\n",
        "Compared to `transcript_df`, what is the time duration that each row captures? How many words on average in each segment?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeEX5hxNpX9E",
        "outputId": "f91fa658-f1b3-4c06-fdc7-4df63b80dcfd"
      },
      "outputs": [],
      "source": [
        "# EXERCISE: what is the time duration that each row captures? \n",
        "   # ...\n",
        "   # write code here\n",
        "   # ...\n",
        "\n",
        "# EXERCISE: How many words on average in each segment?\n",
        "   # ...\n",
        "   # write code here\n",
        "   # ...\n",
        "\n",
        "# EXERCISE: How many tokens on average in each segment?\n",
        "   # ...\n",
        "   # write code here\n",
        "   # ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHEa20XMxJVk"
      },
      "source": [
        "Verify that no text segment contains more than NLP_MAXLEN tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q8ILTKk6Jap"
      },
      "outputs": [],
      "source": [
        "assert (segments_df['tokens'].str.len() > NLP_MAXLEN).sum() == 0, f\"At least one text segment has more than {NLP_MAXLEN} tokens\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwa4Jgihu05f"
      },
      "source": [
        "#### 5.4 Summarize the new longer segments\n",
        "\n",
        "The next step is to pass these text segments to an NLP Summarizer model. For each text segment the model will generate summaries that are not more than `SUMMARY_LENGTH = 128` tokens.\n",
        "\n",
        "The `transformers` library offers a convenient `pipeline` class that wraps up complex code for summarization (and more tasks) into a single API call. [API Reference](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/pipelines#transformers.SummarizationPipeline)\n",
        "\n",
        "Here's an example of how to use `pipeline` to summarize text. Note that you can simply pass in the text to the pipeline, it automatically tokenizes and generates summaries for each input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKaaQExk_FK7",
        "outputId": "1406a188-9f3c-4ab8-89e7-62e0223d16e5"
      },
      "outputs": [],
      "source": [
        "# Define two BART abstracts as strings\n",
        "bart_abstract_1 = \"BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token.\"\n",
        "bart_abstract_2 = \" BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.\"\n",
        "\n",
        "# Combine the two abstracts into a list\n",
        "bart_abstracts = [bart_abstract_1, bart_abstract_2]\n",
        "\n",
        "# Create a summarization pipeline using the specified pre-trained model and device\n",
        "summarizer = pipeline(\"summarization\", model=NLP_ARCH, device=torch.device('cuda:0'))\n",
        "\n",
        "# Use the summarization pipeline to generate summaries of the abstracts, with maximum length 64 and minimum length 20\n",
        "abstracts_summary = summarizer(bart_abstracts, max_length=64, min_length=20)\n",
        "\n",
        "# Print the summaries of the two abstracts\n",
        "print(abstracts_summary[0])\n",
        "print(abstracts_summary[1])\n",
        "\n",
        "# Print the original length and summarized length of the two abstracts\n",
        "print(\"original length: \", len(bart_abstract_1) + len(bart_abstract_2))\n",
        "print(\"summarized length: \", len(abstracts_summary[0]['summary_text']) + len(abstracts_summary[1]['summary_text']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's summarize our video transcriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPeu--k7d3a8",
        "outputId": "0475bf19-b7bb-43fe-ceac-3e6c8cf2156e"
      },
      "outputs": [],
      "source": [
        "# EXERCISE: \n",
        "# Write a function that \n",
        "# - takes in the dataframe generated above, \n",
        "# - generates summaries of 128 tokens or less for each segment, and \n",
        "# - adds them in a new column of the dataframe. Return this dataframe\n",
        "\n",
        "def generate_timestamped_summaries(segments_df: pd.DataFrame, summary_lengths: int = 128):\n",
        "    \"\"\"\n",
        "    Generate summaries of each timestamped segments\n",
        "    \n",
        "    Parameters:\n",
        "        segments_df (DataFrame): The dataframe containing timestamps and text segments\n",
        "        summary_lengths (int): The maximum length of each generated summary\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: A dataframe with timestamps, transcriptions and summaries\n",
        "    \"\"\"\n",
        "       \n",
        "    # ...\n",
        "    # write code here to\n",
        "    # - Extract the sentences from the timestamped transcript\n",
        "    # - Initialize the summarization pipeline\n",
        "    # - Generate summaries for the sentences\n",
        "    # - Add the summaries to the dataframe\n",
        "    # - return the dataframe\n",
        "    # ...\n",
        "\n",
        "\n",
        "summary_df = generate_timestamped_summaries(segments_df, SUMMARY_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fyms064Ud3a8"
      },
      "source": [
        "### 6. View the summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "179Ds1LVBb0l",
        "outputId": "b7b97e1c-9a21-4178-976e-97e54d4c4820"
      },
      "outputs": [],
      "source": [
        "segments_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an0BHx_37T3H"
      },
      "source": [
        "### 7. Format and save the dataframe\n",
        "\n",
        "Looks good! Let's make the timestamps more readable so we can scroll in the video if we need to. Here's a `format_time` helper function.\n",
        "\n",
        "Save the dataframe as a CSV file. This will be useful when we need to cross-check something in the video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJiOjEA37Vxl"
      },
      "outputs": [],
      "source": [
        "def format_time(t):\n",
        "    \"\"\"\n",
        "    Convert a time in seconds to a string in HH:MM:SS format.\n",
        "    \n",
        "    Parameters:\n",
        "        t (str): The time in seconds.\n",
        "    \n",
        "    Returns:\n",
        "        str: The time in HH:MM:SS format.\n",
        "    \"\"\"\n",
        "    t = round(float(t))\n",
        "    hh = t // 3600\n",
        "    t %= 3600\n",
        "    mm = t // 60\n",
        "    ss = t % 60\n",
        "    return f\"{hh:02d}:{mm:02d}:{ss:02d}\"\n",
        "\n",
        "# Format the timestamp columns to be human-readable\n",
        "segments_df['start'] = segments_df['start'].apply(format_time)\n",
        "segments_df['end'] = segments_df['end'].apply(format_time)\n",
        "\n",
        "segments_df.to_csv(f'{FOLDER}/timestamped_summaries.csv', index_label=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Preview `segments_df`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "segments_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrEVEQumCVda"
      },
      "source": [
        "Reading the summary row-by-row is tedious. Concatenate all the individial summaries into a single passage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CdV4eSrCKGQ",
        "outputId": "fb917d39-2da7-4ecf-fb22-ba83b8678f8b"
      },
      "outputs": [],
      "source": [
        "# EXERCISE: Concatenate the segment-summaries into a single paragraph\n",
        "\n",
        "# ...\n",
        "# write code here\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co0EGSFpGd4n"
      },
      "source": [
        "We're busy people, we need a TL;DR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t0SXy8jGRZ3",
        "outputId": "2816bf20-2fb2-46d9-f956-6de3d1cc5647"
      },
      "outputs": [],
      "source": [
        "# EXERCISE: Generate a synopsis of the entire video not exceeding 64 tokens \n",
        "\n",
        "# ...\n",
        "# write code here\n",
        "# ...\n",
        "\n",
        "# HINTS:\n",
        "# - Tokenize the given text passage using a specified tokenizer\n",
        "# - Create a summarization pipeline using a specified pre-trained model and device\n",
        "# - If the number of tokens in the passage is greater than the maximum allowed length,\n",
        "#   split the list of tokens into smaller lists of length NLP_MAXLEN and decode each\n",
        "#   sublist of tokens to create a list of sentences. Generate a summary for each.\n",
        "# - If the number of tokens is within the allowed limit, generate a summary\n",
        "#   directly using the entire passage as input to the summarization pipeline\n",
        "# - Extract the summary text from each generated summary and join them to create\n",
        "#   a single string containing the summary of the text passage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7DmuyIFHChm"
      },
      "source": [
        "Write the TL;DR and paragraph to a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rWT0i2uHHaE"
      },
      "outputs": [],
      "source": [
        "# EXERCISE: Write a file called video_summary.txt that contains the above generated synopsis and passage\n",
        "\n",
        "# ...\n",
        "# write code here\n",
        "#\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "ac7c030bfb229795ab2d5932701f6dc350daee54f9fa64eff282ea6ff3dd9981"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('ptnightly')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
